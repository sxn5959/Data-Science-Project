{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99505f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "og_data = pd.read_csv(r'../Dataset/RTADataset.csv')\n",
    "og_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93379bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48942e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the dataset\n",
    "data = og_data.copy()\n",
    "\n",
    "# maintaining the only required columns in the data set\n",
    "data = data.loc[:, ['Age_band_of_driver', \n",
    "                    'Vehicle_driver_relation', \n",
    "                    'Driving_experience', \n",
    "                    'Types_of_Junction', \n",
    "                    'Road_surface_type', \n",
    "                    'Light_conditions', \n",
    "                    'Weather_conditions',\n",
    "                    'Vehicle_movement', \n",
    "                    'Cause_of_accident', \n",
    "                    'Accident_severity']]\n",
    "\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1efcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the value for accident severity\n",
    "data.Accident_severity = data.Accident_severity.map({'Slight Injury':0, 'Serious Injury':1, 'Fatal injury':2})\n",
    "\n",
    "#dropping the rows of the dataset with NaN and unknown values\n",
    "data.replace(['Unknown', 'unknown'], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d3f5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for presenting the data with respect to the predictors in histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "slight_injury = data[data[\"Accident_severity\"] == 0]\n",
    "serious_injury = data[data[\"Accident_severity\"] == 1]\n",
    "fatal_injury = data[data[\"Accident_severity\"] == 2]\n",
    "\n",
    "# creating the histogram for age band of driver\n",
    "plt.hist(slight_injury[\"Age_band_of_driver\"], bins=7, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Age_band_of_driver\"], bins=7, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Age_band_of_driver\"], bins=7, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the Age Band of Driver with respect to Accident Severity\")\n",
    "plt.xlabel(\"Age Band of Driver\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# creating the histogram for vehicle driver relation\n",
    "plt.hist(slight_injury[\"Vehicle_driver_relation\"], bins=5, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Vehicle_driver_relation\"], bins=5, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Vehicle_driver_relation\"], bins=5, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the Vehicle Driver Relation with respect to Accident Severity\")\n",
    "plt.xlabel(\"Relation\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb19f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# creating the histogram for driving experience\n",
    "plt.hist(slight_injury[\"Driving_experience\"], bins=14, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Driving_experience\"], bins=14, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Driving_experience\"], bins=14, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the Driving Experience with respect to Accident Severity\")\n",
    "plt.xlabel(\"Driving Experience\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f606fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# creating the histogram for types of junction\n",
    "plt.hist(slight_injury[\"Types_of_Junction\"], bins=14, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Types_of_Junction\"], bins=14, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Types_of_Junction\"], bins=14, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the Types of Junction with respect to Accident Severity\")\n",
    "plt.xlabel(\"Types of Junction\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0163b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# creating the histogram for road surface type\n",
    "plt.hist(slight_injury[\"Road_surface_type\"], bins=10, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Road_surface_type\"], bins=10, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Road_surface_type\"], bins=10, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the road surface type with respect to Accident Severity\")\n",
    "plt.xlabel(\"Road Surface Type\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# creating the histogram for light conditions\n",
    "plt.hist(slight_injury[\"Light_conditions\"], bins=8, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Light_conditions\"], bins=8, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Light_conditions\"], bins=8, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the light conditions with respect to Accident Severity\")\n",
    "plt.xlabel(\"Light Conditions\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# creating the histogram for weather conditions\n",
    "plt.hist(slight_injury[\"Weather_conditions\"], bins=14, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Weather_conditions\"], bins=14, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Weather_conditions\"], bins=2, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the weather conditions with respect to Accident Severity\")\n",
    "plt.xlabel(\"Weather Conditions\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "# creating the histogram for vehicle movement\n",
    "plt.hist(slight_injury[\"Vehicle_movement\"], bins=30, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Vehicle_movement\"], bins=30, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Vehicle_movement\"], bins=30, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the vehicle movement with respect to Accident Severity\")\n",
    "plt.xlabel(\"Vehicle Movement\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d178745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 30))\n",
    "\n",
    "# creating the histogram for cause of the accident\n",
    "plt.hist(slight_injury[\"Cause_of_accident\"], bins=40, alpha=1, label=\"Slight Injury\")\n",
    "plt.hist(serious_injury[\"Cause_of_accident\"], bins=40, alpha=1, label=\"Serious Injury\")\n",
    "plt.hist(fatal_injury[\"Cause_of_accident\"], bins=40, alpha=1, label=\"Fatal Injury\", color='red')\n",
    "plt.title(\"Distribution of the cause of the accident  with respect to Accident Severity\")\n",
    "plt.xlabel(\"Cause of the Accident\")\n",
    "plt.ylabel(\"Accidents\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd03f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to check unique categorical features\n",
    "for column in data:\n",
    "    print(column,': ', data[column].unique().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e743bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical features to numeric\n",
    "# mapping the variables\n",
    "data.Age_band_of_driver = data.Age_band_of_driver.map({'Under 18':0, '18-30':1, '31-50': 2, 'Over 51': 3}) \n",
    "data.Driving_experience = data.Driving_experience.map({'No Licence': 0, 'Below 1yr': 1, '1-2yr': 2 , '2-5yr': 3, '5-10yr': 4, 'Above 10yr': 5})\n",
    "data.Road_surface_type = data.Road_surface_type.map({'Earth roads': 0 , 'Gravel roads': 1, 'Asphalt roads with some distress': 2, 'Asphalt roads': 3, 'Other': 4})\n",
    "data.Light_conditions = data.Light_conditions.map({'Daylight': 0, 'Darkness - lights lit': 1, 'Darkness - lights unlit': 2, 'Darkness - no lighting': 3})\n",
    "data.Types_of_Junction = data.Types_of_Junction.map({'No junction': 0,'Crossing': 1, 'Y Shape': 2, 'T Shape': 3, 'X Shape': 4, 'O Shape': 5, 'Other': 6})\n",
    "data.Vehicle_driver_relation = data.Vehicle_driver_relation.map({'Owner':0, 'Employee': 1, 'Other': 2})\n",
    "data.Weather_conditions = data.Weather_conditions.map({'Normal': 0, 'Cloudy': 1, 'Windy':2, 'Fog or mist': 3, 'Raining': 4, 'Raining and Windy': 5, 'Snow': 6, 'Other': 7})\n",
    "\n",
    "\n",
    "data[['Vehicle_movement', 'Cause_of_accident']] = data[['Vehicle_movement', 'Cause_of_accident']].apply(lambda x: pd.factorize(x)[0])\n",
    "print(data.head)\n",
    "#data.Vehicle_movement = data.Vehicle_movement.pd.factorize(['Going straight', 'U-Turn', 'Turnover', 'Waiting to go', 'Moving Backward', 'Getting off', 'Reversing', 'Parked', 'Stopping', 'Overtaking', 'Other', 'Entering a junction'], use_na_sentinel = True)\n",
    "#data.Cause_of_accident = data.Cause_of_accident.pd.factorize(['Moving Backward', 'Overtaking', 'Changing lane to the left', 'Changing lane to the right', 'No priority to vehicle', 'No priority to pedestrian', 'No distancing', 'Getting off the vehicle improperly', 'Overloading', 'Other', 'Overspeed', 'Driving carelessly', 'Driving at high speed', 'Driving to the left', 'Overturning', 'Turnover', 'Driving under the influence of drugs', 'Drunk driving', 'Improper parking'], use_na_sentinel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69242968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the updated dataframe into the new CSV file\n",
    "data.to_csv(r'../Dataset/CleanedDataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##splitting data into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(['Accident_severity'], axis = 1)\n",
    "y = data.Accident_severity\n",
    "\n",
    "#cross validation variable\n",
    "cv_num = 5\n",
    "\n",
    "#random state\n",
    "r_state=1\n",
    "\n",
    "#80/20 test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4470ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reg = LinearRegression() #create linear model\n",
    "reg.fit(X_train, y_train) #fit linear model to train data\n",
    "\n",
    "print('The R2 on the train data set is:', round(r2_score(y_train, reg.predict(X_train)),4))\n",
    "#prediction of linear fitted model trained on training set to test data set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print('The R2 on the test data set is:', round(r2_score(y_test, y_pred),4))\n",
    "\n",
    "### output the results of the linear model fitted using the 'statsmodels'\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#fitting model with statsmodels\n",
    "x2 = sm.add_constant(X) #stasmodels says OLS function doesn't include an intercept so one must be added reference to .add_constant function\n",
    "model = sm.OLS(y, x2)  # return a model object takes (endog, exog, missing, hasconst, kwargs)\n",
    "model_fitted = model.fit()\n",
    "print(model_fitted.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ca1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of influence\n",
    "influence = model_fitted.get_influence()\n",
    "\n",
    "#obtain studentized residuals\n",
    "studentized_residuals = influence.resid_studentized_internal\n",
    "\n",
    "#display studentized residuals\n",
    "print(studentized_residuals)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y, studentized_residuals)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Studentized residuals')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# draw a pairwise scatter plot between the all the columns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   #better heatmap than plt\n",
    "\n",
    "# pair-wise relation between numerical variables\n",
    "sns.set_palette('Reds')\n",
    "sns.pairplot(data, corner=True)\n",
    "plt.show()\n",
    "\n",
    "# draw the low half of the correlation matrix\n",
    "\n",
    "# Generate a mask (selected index of array) to show the lower triangle only\n",
    "corr = data.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=\"YlGnBu\")\n",
    "plt.title('Correlation coefficient among predictors')\n",
    "plt.show()\n",
    "\n",
    "# calculate the VIF of each feature other than the 'crim'\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# calculate VIF\n",
    "vif = pd.DataFrame()\n",
    "#print(vif)\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "#print(vif)\n",
    "vif[\"features\"] = X.columns\n",
    "#print(vif)\n",
    "vif = vif.sort_values(by='VIF',ascending=False)\n",
    "\n",
    "# view\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#recalculating VIF after dropping variables with high VIF's indicating colinearity\n",
    "data_less_pred_corr = data.copy()\n",
    "data_less_pred_corr = data_less_pred_corr.drop(['Cause_of_accident','Types_of_Junction','Light_conditions','Vehicle_movement', 'Weather_conditions'],axis=1)\n",
    "vif_lpc_1 = pd.DataFrame() #VIF with less predictor correlation\n",
    "print(vif_lpc_1)\n",
    "vif_lpc_1[\"VIF\"] = [variance_inflation_factor(data_less_pred_corr.values, i) for i in range(data_less_pred_corr.shape[1])]\n",
    "print(vif_lpc_1)\n",
    "vif_lpc_1[\"features\"] = data_less_pred_corr.columns\n",
    "print(vif_lpc_1)\n",
    "vif_lpc_1 = vif_lpc_1.sort_values(by='VIF',ascending=False)\n",
    "\n",
    "# view\n",
    "vif_lpc_1.round(1)\n",
    "    \n",
    "#these are covered by the previously created array\n",
    "x = data_less_pred_corr\n",
    "x = x.to_numpy()  # pandas series to np array\n",
    "\n",
    "#fitting model with statsmodels\n",
    "x2 = sm.add_constant(x) #stasmodels says OLS function doesn't include an intercept so one must be added reference to .add_constant function\n",
    "model_lpc_1 = sm.OLS(y, x2)  # return a model object takes (endog, exog, missing, hasconst, kwargs)\n",
    "model_lpc_1_fitted = model_lpc_1.fit()\n",
    "\n",
    "print(model_lpc_1_fitted.summary())\n",
    "    \n",
    "#create instance of influence\n",
    "influence = model_lpc_1_fitted.get_influence()\n",
    "\n",
    "#obtain studentized residuals\n",
    "studentized_residuals = influence.resid_studentized_internal\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y,studentized_residuals)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Studentized residuals')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###PCR\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "pca = PCA()\n",
    "pca_transformer_train = pca.fit_transform(scale(X_train))\n",
    "\n",
    "R2 = []\n",
    "model = LinearRegression()\n",
    "\n",
    "#forward subset selection on number of components\n",
    "for i in np.arange(1, len(data.columns)):\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model_selection.cross_val_score(model, pca_transformer_train[:,:i], y_train.ravel(), cv=cv_num, scoring='r2').mean()\n",
    "    R2.append(score)\n",
    "\n",
    "print(f'The R-squared of the model is: {round(model.score(X_test, y_test),5)}')\n",
    "\n",
    "plt.plot(np.array(R2))\n",
    "plt.xlabel('M (number of components)')\n",
    "plt.ylabel('R2')\n",
    "plt.title('PCR model with M chosen by cv=5')\n",
    "plt.xlim(xmin=1)\n",
    "plt.ylim(ymax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa47b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alphas = 10**np.linspace(8,-2,150)*0.5\n",
    "\n",
    "ridgecv = RidgeCV(alphas=alphas, cv=cv_num, scoring='r2')\n",
    "ridgecv.fit(scale(X_train), y_train) \n",
    "\n",
    "ridge2 = Ridge(alpha=len(X)*11498/2)\n",
    "ridge2.fit(scale(X_train), y_train)\n",
    "y_pred = ridge2.predict(scale(X_test))\n",
    "\n",
    "print('Optimal regularization parameter:', ridgecv.alpha_)\n",
    "ridge2.set_params(alpha=ridgecv.alpha_)\n",
    "ridge2.fit(scale(X_train), y_train)\n",
    "print('RIDGE MSE:', mean_squared_error(y_test, ridge2.predict(scale(X_test))))\n",
    "\n",
    "# print the coeffs\n",
    "print(np.round(ridge2.coef_.flatten(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2dc4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Lasso\n",
    "lasso = Lasso(max_iter=5000)\n",
    "coefs = []\n",
    "\n",
    "#for a in alphas*2:\n",
    "#    lasso.set_params(alpha=a)\n",
    "#    lasso.fit(scale(X_train), y_train)\n",
    "#    coefs.append(lasso.coef_)\n",
    "\n",
    "lassocv = LassoCV(alphas=None, cv=10, max_iter=10000) #10 cross folds\n",
    "lassocv.fit(scale(X_train), y_train.values.ravel())\n",
    "lassocv.alpha_\n",
    "\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(scale(X_train), y_train)\n",
    "\n",
    "print('LASSO MSE: ', mean_squared_error(y_test, lasso.predict(scale(X_test))))\n",
    "# print the coeffs\n",
    "print(np.round(lasso.coef_.flatten(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a207a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###PLS\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "names = ['R-Squared']\n",
    "\n",
    "##\n",
    "mle_class = data.value_counts().idxmax()\n",
    "mle_train_accuracy = len(y_train.loc[y_train == mle_class]) / len(y_train)\n",
    "mle_test_accuracy = len(y_test.loc[y_test == mle_class]) / len(y_test)\n",
    "scores = [[mle_train_accuracy, mle_test_accuracy]]\n",
    "##\n",
    "\n",
    "# Define the PLS regression object\n",
    "pls = PLSRegression()\n",
    "\n",
    "# Define the parameter grid for the number of components\n",
    "\n",
    "param_grid = {\"n_components\": range(1, X_train.shape[1])}\n",
    "\n",
    "# Perform a grid search over the parameter grid using cross-validation\n",
    "grid_search = GridSearchCV(pls, param_grid=param_grid, cv=cv_num, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best number of components and the corresponding mean squared error\n",
    "num_components = grid_search.best_params_[\"n_components\"]\n",
    "\n",
    "# Calculate the accuracy\n",
    "grid_search_train_accuracy = grid_search.score(X_train, y_train)\n",
    "grid_search_test_accuracy = grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f'PLS Regression w/ 5-fold CV ({num_components} components) has an R-squared of', f'{round(grid_search_test_accuracy,5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeca40c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Decision Tree\n",
    "\n",
    "#parameters = {'max_depth':(2,4,6,8,10)}\n",
    "parameters = {'max_depth': range(1,10)}\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "#return_train_score=True allows for train score to be tracked later on if needed\n",
    "clf = GridSearchCV(tree.DecisionTreeRegressor(random_state=r_state), parameters, cv=cv_num, scoring=mse_scorer, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "tree_model = clf.best_estimator_\n",
    "print('Optimal value of '+ str(clf.best_params_))\n",
    "\n",
    "p = tree_model.predict(X_train)\n",
    "print(\"Training MSE is: \" + str(mean_squared_error(p, y_train)))\n",
    "\n",
    "p = tree_model.predict(X_test)\n",
    "print(\"Testing MSE is: \" + str(mean_squared_error(p, y_test)))\n",
    "###\n",
    "\n",
    "\n",
    "# Set size for the plot\n",
    "plt.figure(figsize=(10,10))\n",
    "# Plot the tree (use filled for more clarity)\n",
    "plot_tree(tree_model, filled = True, fontsize=8)\n",
    "\n",
    "plt.show() # Display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff62bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f69a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Bagging\n",
    "\n",
    "#bagging = BaggingRegressor(random_state=r_state)\n",
    "#bagging.fit(X=X_train, y=y_train.values.ravel())\n",
    "#p = bagging.predict(X_test)\n",
    "#print(\"Test MSE is: \" + str(mean_squared_error(p, y_test)))\n",
    "\n",
    "parameters = {'n_estimators': np.arange(1, len(data.columns), 1)}\n",
    "#cross validation with parameters and 5-folds\n",
    "clf = GridSearchCV(ensemble.BaggingRegressor(random_state=r_state), parameters, cv=cv_num)\n",
    "clf.fit(X_train, y_train.values.ravel())\n",
    "bagged_tree_model = clf.best_estimator_\n",
    "print (\"Optimal values: \", clf.best_params_)\n",
    "\n",
    "p = bagged_tree_model.predict(X_train)\n",
    "print(\"Training MSE is: \" + str(round(mean_squared_error(p, y_train),4)))\n",
    "\n",
    "p = bagged_tree_model.predict(X_test)\n",
    "print(\"Testing MSE is: \" + str(round(mean_squared_error(p, y_test),4)))\n",
    "\n",
    "feature_importances = np.mean([tree.feature_importances_ for tree in bagged_tree_model.estimators_], axis=0)\n",
    "feature_importances.sort()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(X_train.columns.tolist(), feature_importances)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importances')\n",
    "plt.title(\"Bagging: Importance of Features\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### Random Forests\n",
    "\n",
    "parameters = {'max_depth': range(1,10), 'n_estimators': np.arange(1, len(data.columns), 1)}\n",
    "#cross validation with parameters and 5-folds\n",
    "clf = GridSearchCV(ensemble.RandomForestRegressor(random_state=r_state), parameters, cv=cv_num)\n",
    "clf.fit(X_train, y_train.values.ravel())\n",
    "forest_model = clf.best_estimator_\n",
    "print (\"Optimal values: \", clf.best_params_)\n",
    "\n",
    "p = forest_model.predict(X_train)\n",
    "print(\"Training MSE is: \" + str(round(mean_squared_error(p, y_train),4)))\n",
    "\n",
    "p = forest_model.predict(X_test)\n",
    "print(\"Testing MSE is: \" + str(round(mean_squared_error(p, y_test),4)))\n",
    "\n",
    "\n",
    "feature_importances = np.mean([tree.feature_importances_ for tree in forest_model.estimators_], axis=0)\n",
    "feature_importances.sort()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(X_train.columns.tolist(), feature_importances)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importances')\n",
    "plt.title(\"RF: Importance of Features\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ca527",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###XGB\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier    # python.exe -m pip install xgboost\n",
    "\n",
    "# define the parameter grids to search over\n",
    "params = {\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# perform grid search for XGBoost\n",
    "xgb_model = XGBClassifier(objective='binary:logistic')\n",
    "xgb_grid = GridSearchCV(xgb_model, params, cv=cv_num, scoring='neg_mean_squared_error')\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# fit best XGBoost model and evaluate\n",
    "xgb_best = XGBClassifier(**xgb_grid.best_params_)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "y_train_xgb = xgb_best.predict(X_train)\n",
    "y_test_xgb = xgb_best.predict(X_test)\n",
    "\n",
    "p = xgb_best.predict(X_train)\n",
    "print(\"Training MSE is: \" + str(round(mean_squared_error(p, y_train),4)))\n",
    "\n",
    "p = xgb_best.predict(X_test)\n",
    "print(\"Testing MSE is: \" + str(round(mean_squared_error(p, y_test),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###XGB accuracy\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier    # python.exe -m pip install xgboost\n",
    "\n",
    "# define the parameter grids to search over\n",
    "params = {\n",
    "    #'max_depth': range(1, 10, 1),\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# perform grid search for XGBoost\n",
    "xgb_model = XGBClassifier(objective='binary:logistic')\n",
    "xgb_grid = GridSearchCV(xgb_model, params, cv=cv_num, scoring='accuracy')\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# fit best XGBoost model and evaluate\n",
    "xgb_best = XGBClassifier(**xgb_grid.best_params_)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "y_train_xgb = xgb_best.predict(X_train)\n",
    "y_test_xgb = xgb_best.predict(X_test)\n",
    "\n",
    "p = xgb_best.predict(X_train)\n",
    "print(\"Training Accuracy is: \" + str(round(accuracy_score(p, y_train),4)))\n",
    "\n",
    "p = xgb_best.predict(X_test)\n",
    "print(\"Testing Accuracy is: \" + str(round(accuracy_score(p, y_test),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42239b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###XGB accuracy\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier    # python.exe -m pip install xgboost\n",
    "\n",
    "# define the parameter grids to search over\n",
    "params = {\n",
    "    'max_depth': range(1, 10, 1),\n",
    "    #'max_depth': [5, 7, 9],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# perform grid search for XGBoost\n",
    "xgb_model = XGBClassifier(objective='binary:logistic')\n",
    "xgb_grid = GridSearchCV(xgb_model, params, cv=cv_num, scoring='accuracy')\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# fit best XGBoost model and evaluate\n",
    "xgb_best = XGBClassifier(**xgb_grid.best_params_)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "y_train_xgb = xgb_best.predict(X_train)\n",
    "y_test_xgb = xgb_best.predict(X_test)\n",
    "\n",
    "p = xgb_best.predict(X_train)\n",
    "print(\"Training Accuracy is: \" + str(round(accuracy_score(p, y_train),4)))\n",
    "\n",
    "p = xgb_best.predict(X_test)\n",
    "print(\"Testing Accuracy is: \" + str(round(accuracy_score(p, y_test),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd15911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get feature importances for XGB\n",
    "importances = xgb_best.feature_importances_\n",
    "\n",
    "# create a dataframe of feature importances with column names\n",
    "feature_importances = pd.DataFrame({'feature': list(X_train.columns), 'importance': importances})\n",
    "\n",
    "# sort features by importance in descending order\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# plot feature importances\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(feature_importances['feature'], feature_importances['importance'])\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.xlabel('Features')\n",
    "plt.xlabel('Importance')\n",
    "# hide the top and right axes\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0165b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#new code to check how using all data affects results as suggested/asked by professor after presentation on 5/2/23\n",
    "# deep copy the dataset for og unclean data\n",
    "data_1 = og_data.copy()\n",
    "\n",
    "# replacing the value for accident severity\n",
    "data_1.Accident_severity = data_1.Accident_severity.map({'Slight Injury':0, 'Serious Injury':1, 'Fatal injury':2})\n",
    "\n",
    "#code to check unique categorical features\n",
    "for column in data_1:\n",
    "    print(column,': ', data_1[column].unique().tolist())\n",
    "\n",
    "#converting categorical features to numeric\n",
    "# mapping the variables\n",
    "data_1.Age_band_of_driver = data_1.Age_band_of_driver.map({'Under 18':0, '18-30':1, '31-50': 2, 'Over 51': 3}) \n",
    "data_1.Driving_experience = data_1.Driving_experience.map({'No Licence': 0, 'Below 1yr': 1, '1-2yr': 2 , '2-5yr': 3, '5-10yr': 4, 'Above 10yr': 5})\n",
    "data_1.Road_surface_type = data_1.Road_surface_type.map({'Earth roads': 0 , 'Gravel roads': 1, 'Asphalt roads with some distress': 2, 'Asphalt roads': 3, 'Other': 4})\n",
    "data_1.Light_conditions = data_1.Light_conditions.map({'Daylight': 0, 'Darkness - lights lit': 1, 'Darkness - lights unlit': 2, 'Darkness - no lighting': 3})\n",
    "data_1.Types_of_Junction = data_1.Types_of_Junction.map({'No junction': 0,'Crossing': 1, 'Y Shape': 2, 'T Shape': 3, 'X Shape': 4, 'O Shape': 5, 'Other': 6})\n",
    "data_1.Vehicle_driver_relation = data_1.Vehicle_driver_relation.map({'Owner':0, 'Employee': 1, 'Other': 2})\n",
    "data_1.Weather_conditions = data_1.Weather_conditions.map({'Normal': 0, 'Cloudy': 1, 'Windy':2, 'Fog or mist': 3, 'Raining': 4, 'Raining and Windy': 5, 'Snow': 6, 'Other': 7})\n",
    "\n",
    "\n",
    "data_1[['Time','Day_of_week', 'Sex_of_driver', 'Educational_level', 'Type_of_vehicle', 'Owner_of_vehicle', \n",
    "        'Service_year_of_vehicle', 'Defect_of_vehicle', 'Area_accident_occured', 'Lanes_or_Medians', 'Road_allignment',\n",
    "        'Road_surface_conditions', 'Type_of_collision', 'Number_of_vehicles_involved', 'Number_of_casualties',\n",
    "        'Vehicle_movement', 'Casualty_class', 'Sex_of_casualty', 'Age_band_of_casualty', 'Casualty_severity',\n",
    "        'Work_of_casuality', 'Fitness_of_casuality', 'Pedestrian_movement',\n",
    "        'Cause_of_accident']] = data_1[['Time','Day_of_week', 'Sex_of_driver', 'Educational_level', 'Type_of_vehicle',\n",
    "                                        'Owner_of_vehicle', 'Service_year_of_vehicle', 'Defect_of_vehicle',\n",
    "                                        'Area_accident_occured', 'Lanes_or_Medians', 'Road_allignment',\n",
    "                                        'Road_surface_conditions', 'Type_of_collision', 'Number_of_vehicles_involved',\n",
    "                                        'Number_of_casualties', 'Vehicle_movement', 'Casualty_class', 'Sex_of_casualty',\n",
    "                                        'Age_band_of_casualty','Casualty_severity','Work_of_casuality','Fitness_of_casuality',\n",
    "                                        'Pedestrian_movement', 'Cause_of_accident']].apply(lambda x: pd.factorize(x)[0])\n",
    "print(data_1.head)\n",
    "\n",
    "##splitting not cleaned data into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_og = data_1.drop(['Accident_severity'], axis = 1)\n",
    "y_og = data_1.Accident_severity\n",
    "\n",
    "##new testtrain split for all data: 80/20 test train split\n",
    "X_og_train, X_og_test, y_og_train, y_og_test = train_test_split(X_og, y_og, train_size = 0.8, random_state = r_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b41b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### XGBoost accuracy with all data not just cleaned. \n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier    # python.exe -m pip install xgboost\n",
    "\n",
    "# define the parameter grids to search over\n",
    "params = {\n",
    "    'max_depth': range(1, 10, 1),\n",
    "    #'max_depth': [5, 7, 9],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# perform grid search for XGBoost\n",
    "xgb_og_model = XGBClassifier(objective='binary:logistic')\n",
    "xgb_og_grid = GridSearchCV(xgb_og_model, params, cv=cv_num, scoring='accuracy')\n",
    "\n",
    "xgb_og_grid.fit(X_og_train, y_og_train)\n",
    "\n",
    "# fit best XGBoost model and evaluate\n",
    "xgb_og_best = XGBClassifier(**xgb_og_grid.best_params_)\n",
    "xgb_og_best.fit(X_og_train, y_og_train)\n",
    "\n",
    "y_og_train_xgb = xgb_og_best.predict(X_og_train)\n",
    "y_og_test_xgb = xgb_og_best.predict(X_og_test)\n",
    "\n",
    "p_og = xgb_og_best.predict(X_og_train)\n",
    "print(\"XGB Training Accuracy with og data is: \" + str(round(accuracy_score(p_og, y_og_train),4)))\n",
    "\n",
    "p_og = xgb_og_best.predict(X_og_test)\n",
    "print(\"XGB Testing Accuracy with og data is: \" + str(round(accuracy_score(p_og, y_og_test),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not used\n",
    "###SVC \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "import copy\n",
    "#plot_svc function from class notes\n",
    "# Define a function to plot a classifier with support vectors.\n",
    "#used in gridsearchcv\n",
    "def plot_svc(svc, X, y, h=0.02, pad=1):\n",
    "    # Set the limits for the x and y axes\n",
    "    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n",
    "    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n",
    "    \n",
    "    # Create a meshgrid for the plot\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict the class labels for the meshgrid points\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundaries using a contour plot\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "\n",
    "    # Plot the data points with different colors for different classes\n",
    "    plt.scatter(X[:,0], X[:,1], s=100, c=y, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Mark support vectors\n",
    "    sv = svc.best_estimator_.support_vectors_ ##can't use .support_vectors_ insided of GridSearchCV\n",
    "    \n",
    "    plt.scatter(sv[:,0], sv[:,1], color='k', marker='x', s=80, linewidths=1)\n",
    "    \n",
    "    # Set the limits and labels for the plot\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "\n",
    "\n",
    "#used outside gridsearchcv\n",
    "def plot_svc1(svc, X, y, h=0.02, pad=0.25):\n",
    "    \n",
    "    # Set the limits for the x and y axes\n",
    "    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n",
    "    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n",
    "    \n",
    "    # Create a meshgrid for the plot\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict the class labels for the meshgrid points\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundaries using a contour plot\n",
    "    # plt.figure(figsize=(5, 4))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "\n",
    "    # Plot the data points with different colors for different classes\n",
    "    plt.scatter(X[:,0], X[:,1], s=100, c=y, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Mark support vectors\n",
    "    sv = svc.support_vectors_\n",
    "    plt.scatter(sv[:,0], sv[:,1], color='k', marker='x', s=80, linewidths=1)\n",
    "    \n",
    "    # Set the limits and labels for the plot\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#RBF kernel\n",
    "rbf_parameters = [{'C': [0.1, 1, 10, 100],\n",
    "                   'gamma': [0.1, 0.5, 1, 5]}]\n",
    "\n",
    "clf_rbf = GridSearchCV(SVC(kernel='rbf'), rbf_parameters, cv=cv_num, scoring='accuracy', return_train_score=True)\n",
    "clf_rbf.fit(X_train, y_train)\n",
    "\n",
    "# the best C\n",
    "print(f'The optimal value of C is: {clf_rbf.best_params_[\"C\"]}')\n",
    "print(f'The optimal value of gamma is: {clf_rbf.best_params_[\"gamma\"]}')\n",
    "\n",
    "#plot_svc(clf_rbf, X_test, y_test)\n",
    "#plot_svc(clf_rbf_best, X_train, y_train)\n",
    "print(f'The accuracy of the model on the train set is: {clf_rbf.best_estimator_.score(X_train, y_train)}')\n",
    "print(f'The accuracy of the model on the test set is: {clf_rbf.best_estimator_.score(X_test, y_test)}')\n",
    "\n",
    "clf_rbf_best = SVC(C = clf_rbf.best_params_[\"C\"], kernel = 'rbf', gamma = clf_rbf.best_params_[\"gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53daa422",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#RBF kernel accuracy with all data not just cleaned.\n",
    "rbf_parameters = [{'C': [0.1, 1, 10, 100],\n",
    "                   'gamma': [0.1, 0.5, 1, 5]}]\n",
    "\n",
    "clf_og_rbf = GridSearchCV(SVC(kernel='rbf'), rbf_parameters, cv=cv_num, scoring='accuracy', return_train_score=True)\n",
    "clf_og_rbf.fit(X_og_train, y_og_train)\n",
    "\n",
    "# the best C\n",
    "print(f'The optimal value of C is: {clf_og_rbf.best_params_[\"C\"]}')\n",
    "print(f'The optimal value of gamma is: {clf_og_rbf.best_params_[\"gamma\"]}')\n",
    "\n",
    "print(f'The accuracy of the model on the train set is: {clf_og_rbf.best_estimator_.score(X_og_train, y_og_train)}')\n",
    "print(f'The accuracy of the model on the test set is: {clf_og_rbf.best_estimator_.score(X_og_test, y_og_test)}')\n",
    "\n",
    "clf_rbf_best_og = SVC(C = clf_og_rbf.best_params_[\"C\"], kernel = 'rbf', gamma = clf_og_rbf.best_params_[\"gamma\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
